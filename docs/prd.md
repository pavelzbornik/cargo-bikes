# Cargo Bikes Documentation Vault Brownfield Enhancement PRD

## Intro Project Analysis and Context

### Existing Project Overview

#### Analysis Source

- Document-project output available at: `docs/brownfield-architecture.md` (Generated by Mary, the Analyst).

#### Current Project State

- The project is a "Docs-as-Code" system where a vault of Markdown files with structured YAML frontmatter serves as the database. A suite of Python scripts automates content generation (e.g., README tables) and validation. The entire system is deployed as a static site to GitHub Pages via an automated CI/CD pipeline.

### Available Documentation Analysis

- A comprehensive `document-project` analysis is available. I will use this as the foundation for our planning.
- **Available Documentation:**
  - [x] Tech Stack Documentation
  - [x] Source Tree/Architecture
  - [x] Coding Standards
  - [x] API Documentation (CLI-based)
  - [x] Technical Debt Documentation

### Enhancement Scope Definition

#### Enhancement Type

- [x] New Feature Addition (Agentic Automation)
- [x] Major Feature Modification (Data handling and synchronization)
- [x] Technology Stack Upgrade (Introduction of a formal database and Pydantic AI)

#### Enhancement Description

This project will transform the existing Markdown-based vault into a dynamic knowledge base powered by a central database. The enhancement involves three core pillars:

1. Establishing a database as the single source of truth for all bike, brand, and component data.
2. Developing scripts for two-way synchronization between the database and the Markdown files, which will now serve as a presentation layer.
3. Replacing existing Copilot prompts with autonomous Pydantic AI agents to automate data fetching and maintenance, integrated into the CI/CD pipeline.

#### Impact Assessment

- [x] **Major Impact (architectural changes required)**: This enhancement introduces a new data persistence layer, changes the data flow, and adds a new agentic automation system.

### Goals and Background Context

#### Goals

- Establish a scalable, centralized database as the single source of truth for all vault data.
- Automate the generation of YAML frontmatter and structured Markdown content (e.g., spec tables) from the database.
- Enable two-way data synchronization between the database and the Markdown files, with conflict resolution.
- Implement autonomous Pydantic AI agents to replace manual data fetching and updates.
- Integrate agentic automation into the GitHub Actions CI/CD pipeline for continuous data freshness.

#### Background Context

The current system relies on manually edited Markdown files with YAML frontmatter. This "Git-as-a-database" approach is effective for small-scale projects but presents challenges in data consistency, scalability, and maintenance. As the volume of data grows, manual updates become error-prone and time-consuming.

This enhancement addresses these challenges by introducing a robust architectural pattern. A central database will ensure data integrity, while autonomous agents will eliminate the need for manual data fetching and updates. This will significantly improve the accuracy, scalability, and long-term viability of the knowledge base.

### Change Log

| Date       | Version | Description       | Author |
| ---------- | ------- | ----------------- | ------ |
| 2024-05-22 | 0.1     | Initial PRD draft | John   |

## Requirements

### Functional

1. **FR1: Database Schema Definition:** A database schema shall be designed and implemented to store all data currently defined in the `docs/schema/` files (e.g., `BIKE_SPECS_SCHEMA.md`, `MANUFACTURER_SCHEMA.md`).
2. **FR2: Data Migration:** A one-time migration script shall be created to parse all existing YAML frontmatter from the `vault/` and populate the new database.
3. **FR3: Database-to-Markdown Synchronization:** A script shall be developed to read from the database and automatically generate/update both the YAML frontmatter and a structured "Specifications" table within the body of the corresponding Markdown files.
4. **FR4: Markdown-to-Database Synchronization:** A script shall be developed to detect manual changes in the Markdown files (both frontmatter and the generated table) and write those changes back to the database.
5. **FR5: Conflict Resolution:** A mechanism shall be implemented to detect and handle synchronization conflicts (e.g., when a file and the database are updated simultaneously). The system should log conflicts and, by default, prioritize database changes unless configured otherwise.
6. **FR6: Agent Implementation:** The functionality described in the existing `.github/prompts/` files (e.g., `fetch-bikes`, `fetch-reseller-prices`) shall be re-implemented as autonomous agents using the Pydantic AI framework. These agents will interact with the web and write their findings directly to the database.
7. **FR7: CI/CD Integration:** The new Pydantic AI agents and synchronization scripts shall be integrated into a GitHub Actions workflow that can be triggered both on a schedule and manually.

### Non-Functional

1. **NFR1: Database Technology:** The database shall be a lightweight, file-based system (e.g., SQLite) to ensure portability and ease of use within the GitHub Actions environment.
2. **NFR2: Idempotency:** All synchronization scripts must be idempotent, meaning they can be run multiple times without changing the result beyond the initial intended state.
3. **NFR3: Logging and Traceability:** All agent actions and data synchronization processes must be thoroughly logged to provide a clear audit trail of what was changed, when, and by which process.
4. **NFR4: Performance:** The synchronization scripts should complete within a reasonable timeframe suitable for a CI/CD pipeline (e.g., under 5 minutes for a full vault sync).

### Compatibility Requirements

1. **CR1: Preserve Manual Content:** The synchronization process **must not** alter or delete any manually written "free-form" Markdown content within the notes. Only the YAML frontmatter and the specifically designated auto-generated content blocks (like the spec table) should be affected.
2. **CR2: Maintain Existing Deployment:** The new data pipeline must not interfere with the existing `mkdocs` build and GitHub Pages deployment workflow. It should act as a preceding step that prepares the content.
3. **CR3: Schema Adherence:** The database schema and synchronization logic must fully support all fields and structures defined in the existing `docs/schema/` documents.

## Technical Constraints and Integration Requirements (Final Revision)

### Integration Approach (MD-First with Fenced Blocks)

- **Database Integration Strategy**: The `.md` files in the `vault/` remain the **source of truth**. The SQLite database serves as a structured, composable replica. The data flow is unidirectional: `MD -> DB -> MD`.

### Data Workflow: "MD-First with Fenced Blocks"

1. **Phase 1: Hydration (`MD files -> Database`)**
   - A script will parse all `.md` files in the `vault/`.
   - A **custom linter** will validate that each file has a valid YAML frontmatter block.
   - The script will **only read the YAML frontmatter** from each file to populate (hydrate) the SQLite database. The free-form Markdown body will not be parsed into the database.

2. **Phase 2: Enrichment (`Agents -> Database`)**
   - Autonomous **Pantic AI agents** will perform their tasks (e.g., fetching new data).
   - These agents will write their findings **directly to the SQLite database**, updating the structured data records.

3. **Phase 3: Projection (`Database -> MD files`)**
   - After the agents have updated the database, the final projection script will run.
   - This script reads the enriched data records from the database.
   - For each corresponding `.md` file, it performs two targeted updates:
     1. It **completely replaces the YAML frontmatter** with the new, updated data from the database.
     2. It locates a specific, comment-fenced block in the Markdown body (e.g., `<!-- BIKE_SPECS_TABLE_START -->...<!-- BIKE_SPECS_TABLE_END -->`) and **replaces the content within that block** with a newly generated table or list based on the database record.
   - **CRITICAL**: All other content outside the YAML frontmatter and the designated comment-fenced blocks will be left untouched and perfectly preserved.

### Risk Assessment and Mitigation (Revised)

- **Technical Risks**:
  - **Risk**: Custom Markdown linter may not be robust enough.
  - **Mitigation**: The linter will be developed with an extensive test suite covering all known valid and invalid file structures. It will produce clear, actionable error messages.
- **Integration Risks**:
  - **Risk**: A bug in the projection script could corrupt or delete manual content.
  - **Mitigation**: This risk is now significantly lower. The script's logic will be strictly confined to replacing the frontmatter and the content between specific HTML comment markers. The test suite will include complex Markdown files to ensure that only these designated areas are ever modified.
- **Deployment Risks**:
  - **Risk**: CI/CD pipeline could become too slow.
  - **Mitigation**: The scripts will be optimized for performance. The database hydration step can be made incremental, only updating records for files that have changed since the last run.

## Epic and Story Structure

### Epic Approach

**Epic Structure Decision**: This project will be structured into **three sequential epics**. This approach allows us to first build the foundational data engine (Epic 1), then prove its value with a single automated agent (Epic 2), and finally integrate the entire system into the CI/CD pipeline for full automation (Epic 3). This phased delivery ensures we can validate each major component of the new architecture before building upon it. A fourth epic is proposed for future work to complete the agent fleet.

## Epic List

1. **Epic 1: Database Foundation & Core Synchronization Engine**
   - **Goal:** Establish the SQLite database and implement the core `MD -> DB -> MD` two-way synchronization logic. This epic delivers the foundational data pipeline, fully testable but manually triggered.

2. **Epic 2: Agentic Automation - The Price Updater Agent**
   - **Goal:** Develop the first autonomous Pydantic AI agent (`fetch-reseller-prices`) that reads from and writes to the database established in Epic 1. This epic proves the agentic enrichment part of the architecture.

3. **Epic 3: CI/CD Integration for Continuous Data Freshness**
   - **Goal:** Integrate the entire data pipeline (linter, hydration, agents, projection) into the existing GitHub Actions workflow. This epic makes the system fully autonomous, running on a schedule to keep the vault's content up-to-date.

4. **Epic 4 (Future Work): Full Agent Fleet Implementation**
   - **Goal:** Convert all remaining prompts (`fetch-bikes`, `generate-brand-index-pages`, etc.) into autonomous Pydantic AI agents, completing the full automation vision.

## Epic 1: Database Foundation & Core Synchronization Engine

**Epic Goal**: Establish the SQLite database schema, implement the core `MD -> DB -> MD` synchronization logic, and create the custom Markdown linter. This epic delivers the foundational data pipeline, fully testable but manually triggered, ready for agentic integration in the next epic.

---

### Story 1.1: Database Schema and Initialization

**As a** System Architect,
**I want** a SQLite database schema that perfectly mirrors the structures in `BIKE_SPECS_SCHEMA.md` and `MANUFACTURER_SCHEMA.md`,
**so that** we have a structured, queryable data store for all vault content.

**Acceptance Criteria:**

1. A Python script (`scripts/database/init_db.py`) is created that generates an empty SQLite database file (`vault.db`).
2. The script defines tables for `bikes`, `brands`, `components`, and `resellers` using a library like SQLAlchemy.
3. The table columns and relationships directly correspond to all fields defined in the `docs/schema/` markdown files.
4. The generated database file is added to `.gitignore` to prevent it from being committed to the repository.
5. Unit tests are created to verify that the schema is created correctly.

---

### Story 1.2: Custom Markdown Structural Linter

**As a** Developer,
**I want** a custom linter script that validates the structure of my Markdown notes,
**so that** I can ensure they are parseable before hydrating the database.

**Acceptance Criteria:**

1. A Python script (`scripts/linters/validate_structure.py`) is created.
2. The linter checks that every `.md` file in `vault/notes/` contains a valid YAML frontmatter block.
3. For bike notes, it verifies that a `<!-- BIKE_SPECS_TABLE_START -->` block exists in the body.
4. The script can be run from the command line and reports a list of invalid files with clear error messages.
5. If all files are valid, the script exits with a success code. If any are invalid, it exits with an error code.
6. Unit tests are created to validate both passing and failing file structures.

---

### Story 1.3: Hydration Script (MD files -> Database)

**As a** Data Engineer,
**I want** a script that reads YAML frontmatter from all valid Markdown files and populates the SQLite database,
**so that** I can create a structured replica of the vault's content.

**Acceptance Criteria:**

1. A Python script (`scripts/database/hydrate.py`) is created.
2. The script first runs the custom linter from Story 1.2. If the linter fails, the script aborts with an error.
3. The script recursively scans `vault/notes/` and parses the YAML frontmatter of each valid `.md` file.
4. Data from the frontmatter is inserted or updated into the corresponding tables in `vault.db`.
5. The script is idempotent: running it multiple times on an unchanged vault results in the same database state.
6. The script logs its progress, reporting the total number of files processed and records created/updated.
7. Unit tests are created to verify the hydration logic for bikes, brands, and resellers.

---

### Story 1.4: Projection Script (Database -> MD files)

**As a** Data Engineer,
**I want** a script that updates the Markdown files based on the data in the SQLite database,
**so that** the files reflect the enriched, canonical data.

**Acceptance Criteria:**

1. A Python script (`scripts/database/project.py`) is created.
2. The script reads data for a specific bike (or all bikes) from the `vault.db`.
3. For each bike, it locates the corresponding `.md` file.
4. The script **completely replaces the YAML frontmatter** in the file with the data from the database.
5. The script generates a Markdown table of the bike's specifications.
6. The script **replaces the content within the `<!-- BIKE_SPECS_TABLE_START -->` and `<!-- BIKE_SPECS_TABLE_END -->` markers** with the newly generated table.
7. **CRITICAL**: All other Markdown content outside the frontmatter and this specific block is preserved without any modification.
8. Unit tests are created to verify that manual content is preserved while machine-managed sections are correctly updated.

## Epic 2: Agentic Automation - The Price Updater Agent

**Epic Goal**: Develop the first autonomous Pydantic AI agent (`fetch-reseller-prices`) that reads from and writes to the database established in Epic 1. This epic proves the agentic enrichment part of the architecture and delivers our first piece of automated data maintenance.

---

### Story 2.1: Agent Scaffolding and CLI

**As a** Developer,
**I want** to create the basic Pydantic AI agent structure with a configurable CLI entry point,
**so that** I have a runnable foundation for the reseller-fetching agent.

**Acceptance Criteria:**

1. A new Python package is created at `scripts/agents/price_updater/`.
2. A `cli.py` is created using a library like Typer or Click, which can be invoked from the command line.
3. The CLI accepts arguments, such as a specific brand to process or a `--dry-run` flag.
4. A basic Pydantic AI agent is initialized within the package, with logging configured to output to the console.
5. Running the CLI command successfully initializes the agent and logs a startup message.
6. Unit tests are created to verify the CLI command and its basic functionality.

---

### Story 2.2: Web Fetching and Parsing Logic

**As an** Agent,
**I want** to be given a reseller's website URL, fetch all bike listings, and parse them into a structured format,
**so that** I have the raw data needed to update the database.

**Acceptance Criteria:**

1. The agent can perform a web search (using DuckDuckGo tools) to find a reseller's bike catalog if only a name is provided.
2. The agent can fetch the HTML content from the catalog pages, correctly handling pagination to retrieve all bike listings.
3. The agent parses the HTML to extract key information for each bike: brand, model, price, currency, availability, and product URL.
4. The extracted raw data for a given reseller is saved as a Markdown table in a temporary audit file (e.g., `docs/temp-reseller-fetches/[RESELLER_NAME]-[DATE].md`), as specified in the original prompt.
5. The web-fetching logic is modular and can be tested independently with mock HTML data.

---

### Story 2.3: Database Matching and Update Logic

**As an** Agent,
**I want** to take the parsed reseller data and write it to the SQLite database,
**so that** our central data store is enriched with the latest pricing and availability.

**Acceptance Criteria:**

1. The agent can connect to the SQLite database (`vault.db`) created in Epic 1.
2. For each bike parsed from the reseller, the agent attempts to match it to an existing bike record in the database using brand and model name.
3. If a match is found, the agent updates the `resellers` table associated with that bike, either adding a new reseller entry or updating an existing one.
4. If a bike from the reseller cannot be matched, it is logged in the audit file as "unmatched" for manual review.
5. The database update logic is transactional; a failure for one bike does not prevent other valid updates from being committed.
6. The `updated_at` timestamp for any modified bike or reseller record is updated in the database.
7. Unit tests are created to verify the database matching and update logic using mock data.

---

### Story 2.4: End-to-End Agent Execution

**As a** Data Steward,
**I want** to run the price updater agent for a specific reseller via the CLI and have it update the database,
**so that** I can manually trigger a full data refresh cycle.

**Acceptance Criteria:**

1. Running the CLI command (e.g., `python -m scripts.agents.price_updater.cli update-reseller "BikeShop.com"`) triggers the full end-to-end process: search, fetch, parse, match, and database update.
2. After the agent run is complete, the `vault.db` contains the new or updated reseller information.
3. The temporary audit file (`docs/temp-reseller-fetches/...`) is correctly generated and filled out.
4. The console log provides a clear summary of the run, including the number of bikes processed, updated, and unmatched.
5. Running the `project.py` script from Story 1.4 after the agent has run will cause the relevant `.md` files to be updated with the new reseller info.

## Epic 3: CI/CD Integration for Continuous Data Freshness

**Epic Goal**: Integrate the entire data pipeline (linter, hydration, agents, projection) into the existing GitHub Actions workflow. This epic makes the system fully autonomous, running on a schedule to keep the vault's content up-to-date and completing the core vision of this enhancement.

---

### Story 3.1: Master Pipeline Orchestration Script

**As a** DevOps Engineer,
**I want** a single master script that executes the entire `Lint -> Hydrate -> Enrich -> Project` data pipeline in the correct sequence,
**so that** I have a simple, reliable entry point to run the full update process from my CI/CD workflow.

**Acceptance Criteria:**

1. A new master script is created at `scripts/run_pipeline.py`.
2. The script imports and calls the necessary functions from the scripts created in Epics 1 and 2 in the following order:
   a. Custom Linter (`validate_structure.py`)
   b. Database Hydration (`hydrate.py`)
   c. Agent Execution (e.g., `price_updater` agent)
   d. Database Projection (`project.py`)
3. The script includes robust error handling: if any step fails, the entire pipeline must stop and exit with a non-zero status code.
4. The script accepts a `--dry-run` argument that is passed down to all sub-processes to simulate a run without making changes.
5. The script provides clear logging, indicating the start and successful completion of each major phase.

---

### Story 3.2: Manual CI/CD Workflow Integration

**As a** Developer,
**I want** to add a manually-triggered (`workflow_dispatch`) job to my GitHub Actions workflow that runs the master pipeline script,
**so that** I can test the full automation cycle in a real CI environment and trigger updates on demand.

**Acceptance Criteria:**

1. The `.github/workflows/deploy-pages.yml` workflow is modified to include a `workflow_dispatch` trigger.
2. A new job, `update-vault-data`, is added to the workflow, configured to run _before_ the existing `build` job.
3. This job checks out the repository, sets up the Python environment, installs all dependencies, and executes the master pipeline script (`scripts/run_pipeline.py`).
4. After the script runs, the job checks for any modified `.md` files.
5. If changes are detected, the job automatically commits and pushes them back to the `main` branch with a standardized commit message (e.g., "chore: Auto-update vault data").
6. The subsequent `build` and `deploy` jobs are configured to use the newly committed content.
7. The workflow succeeds even if the pipeline script makes no changes to the files.

---

### Story 3.3: Scheduled Automation for Continuous Freshness

**As a** Data Steward,
**I want** the data update pipeline to run automatically on a regular schedule,
**so that** the vault's content is always kept fresh without manual intervention.

**Acceptance Criteria:**

1. The `.github/workflows/deploy-pages.yml` workflow is updated to add a `schedule` trigger (e.g., a weekly `cron` job).
2. The scheduled workflow successfully executes the `update-vault-data` job from Story 3.2.
3. If the scheduled run results in data changes, a commit is automatically pushed to the repository.
4. Following a successful data update, the `build` and `deploy` jobs run automatically, deploying the new content to GitHub Pages.
5. The scheduled workflow has the correct permissions (e.g., using `GITHUB_TOKEN`) to commit changes back to the repository.
